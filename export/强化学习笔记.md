# 强化学习系统笔记

<Section id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_26f50233">
## 第一章：强化学习基础与问题设定

<Introduction id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_26f50233_field_55ba64e2">
本章目标是建立强化学习的基本概念与数学表述，明确常用记号、基本目标与问题设定，为后续算法与理论推导打下统一基础。内容包括智能体-环境交互、MDP 定义、策略与回报、价值函数、贝尔曼方程与最优性条件、常见设定及小实验建议。
</Introduction>

### 总结
<Summary id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_26f50233_field_85d472c9">
本节建立了策略、状态价值与动作价值的基本定义，并证明了状态价值与动作价值之间的关系 $V^{\pi}(s)=\sum_a\pi(a\mid s)Q^{\pi}(s,a)$（离散动作空间）。
</Summary>

<Exercises>
#### 证明题
<ExampleItem id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_26f50233_example_d0f928e5">

**题目：**
<Question id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_26f50233_example_d0f928e5_field_67e6ea61">
证明对于任意策略 $\pi$，状态价值函数 $V^{\pi}$ 与动作价值函数 $Q^{\pi}$ 满足 $V^{\pi}(s)=\sum_{a}\pi(a\mid s)Q^{\pi}(s,a)$。
</Question>

**答案：**
<Answer id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_26f50233_example_d0f928e5_field_65988d3d">
结论成立：$V^{\pi}(s)=\sum_{a}\pi(a\mid s)Q^{\pi}(s,a)$。
</Answer>

**证明步骤：**
<Proof id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_26f50233_example_d0f928e5_field_4239d81d">
证明（假设动作空间为可数离散集合）：

步骤1：写出定义。
- 状态价值函数的定义为
$$V^{\pi}(s)=\mathbb{E}_{\pi}[G_t\mid S_t=s],$$
其中 $G_t$ 是从时间 $t$ 起的折扣回报。 
- 动作价值函数的定义为
$$Q^{\pi}(s,a)=\mathbb{E}_{\pi}[G_t\mid S_t=s, A_t=a].$$

步骤2：应用全期望公式（law of total expectation）。
根据全期望公式，对于随机变量 $X$ 和划分事件由随机变量 $Y$ 给出，有
$$\mathbb{E}[X]=\mathbb{E}[\,\mathbb{E}[X\mid Y]\.].$$
在此令 $X=G_t$，$Y=A_t$ 并在条件 $S_t=s$ 下应用条件全期望，可得
$$V^{\pi}(s)=\mathbb{E}_{\pi}[G_t\mid S_t=s]=\mathbb{E}_{\pi}\big[\,\mathbb{E}_{\pi}[G_t\mid S_t=s,A_t]\;\big|\;S_t=s\big].$$

步骤3：用动作概率展开条件期望。
由于在给定 $S_t=s$ 的条件下，$A_t$ 的分布由策略 $\pi$ 给出，即 $\mathbb{P}(A_t=a\mid S_t=s)=\pi(a\mid s)$，且动作集合为可数，条件期望对 $A_t$ 的外层期望等价于对动作的加权和：
$$V^{\pi}(s)=\sum_{a}\mathbb{P}(A_t=a\mid S_t=s)\,\mathbb{E}_{\pi}[G_t\mid S_t=s,A_t=a].$$

步骤4：代入 $Q^{\pi}$ 的定义并整理，得到结论。
根据步骤1 中 $Q^{\pi}(s,a)=\mathbb{E}_{\pi}[G_t\mid S_t=s,A_t=a]$，代入上式得
$$V^{\pi}(s)=\sum_{a}\pi(a\mid s)\,Q^{\pi}(s,a).$$

步骤5：说明适用性。
若动作空间为连续集合，应将求和替换为对动作的积分，即
$$V^{\pi}(s)=\int_a\pi(a\mid s)Q^{\pi}(s,a)\,da.$$

因此，对于任意策略 $\pi$（在离散动作空间情形），有
$$V^{\pi}(s)=\sum_{a}\pi(a\mid s)Q^{\pi}(s,a),$$
证明完毕。
</Proof>

</ExampleItem>

</Exercises>

</Section>

---

<Section id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_4fc69764">
## 第二章：动态规划与贝尔曼分析

<Introduction id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_4fc69764_field_023127b9">
本章介绍基于已知环境模型（MDP）的解析方法——动态规划（DP），并深入分析贝尔曼期望方程与贝尔曼最优方程。目标包括推导贝尔曼关系、证明迭代策略评估的收敛性、掌握策略/价值迭代并比较其复杂度、讨论同步/异步更新及近似 DP 的基本思路等。
</Introduction>

### 总结
<Summary id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_4fc69764_field_cd404a8a">
本节包含对贝尔曼期望方程不动点的证明练习：证明若 $v$ 是 $T^{\pi}$ 的不动点，则满足贝尔曼期望方程；并在有限 MDP 且 $0\le\gamma<1$ 时证明不动点唯一性（给出收缩映射与矩阵可逆性两种论证）。
</Summary>

<Exercises>
#### 证明题
<ExampleItem id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_4fc69764_example_afed14d6">

**题目：**
<Question id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_4fc69764_example_afed14d6_field_67e0f620">
证明：若 $v$ 是 $T^{\pi}$ 的不动点（$v = T^{\pi} v$），那么 $v$ 满足贝尔曼期望方程，并且在有限 MDP 下该不动点唯一。
</Question>

**答案：**
<Answer id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_4fc69764_example_afed14d6_field_1ff508c9">
结论：若 $v$ 满足 $v = T^{\pi}v$，则 $v$ 满足贝尔曼期望方程 $v = r^{\pi} + \gamma P^{\pi} v$。在有限 MDP 且 $0\le\gamma<1$ 时，该不动点唯一（可由收缩映射定理或矩阵可逆性论证）。
</Answer>

**证明步骤：**
<Proof id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_4fc69764_example_afed14d6_field_7b8dc5e6">
**证明：**

**说明与记号**：令 $T^{\pi}$ 为策略 $\pi$ 下的贝尔曼算子，其对任意状态函数 $v$ 的定义为

$$
T^{\pi} v(s) 
= \sum_a \pi(a\mid s)\Big(R(s,a) + \gamma\sum_{s'} P(s'\mid s,a)\, v(s')\Big).
$$

我们要证明两点：
(1) 若 $v$ 为 $T^{\pi}$ 的不动点（即 $v=T^{\pi}v$），则 $v$ 满足逐状态形式的贝尔曼期望方程；
(2) 在有限 MDP 且 $0\le\gamma<1$ 时，该不动点唯一。


步骤1：从不动点到逐状态的贝尔曼期望方程

(1) 假设 $v$ 满足 $v=T^{\pi}v$。则对任意状态 $s$，由 $T^{\pi}$ 的定义有

$$
v(s)=T^{\pi}v(s)=\sum_a \pi(a\mid s)\Big(R(s,a) + \gamma\sum_{s'} P(s'\mid s,a)\, v(s')\Big).
$$

(2) 记策略下的即时奖励向量和转移矩阵为

$$
r^{\pi}(s)=\sum_a \pi(a\mid s) R(s,a),\qquad P^{\pi}(s'\mid s)=\sum_a \pi(a\mid s) P(s'\mid s,a).
$$

(3) 将上式利用 (2) 合并期望项，得到逐状态的贝尔曼期望方程：

$$
v(s)=r^{\pi}(s)+\gamma \sum_{s'} P^{\pi}(s'\mid s)\, v(s').
$$

这即是逐状态形式的贝尔曼期望方程，完成第一部分证明。

步骤2：向量/矩阵形式的等价表达

(1) 将步骤1 中对所有 $s$ 的等式同时写出，即可得到向量形式：

$$
v = r^{\pi} + \gamma P^{\pi} v.
$$

(2) 该等式等价于

$$
(I - \gamma P^{\pi}) v = r^{\pi}.
$$

因此任一 $T^{\pi}$ 的不动点必须满足上述线性方程组。

步骤3：在有限 MDP 且 $0\le\gamma<1$ 时的不动点唯一性（方法一：收缩映射）

(1) 我们在无穷范数（切比雪夫范数）上考察算子 $T^{\pi}$。记对任意函数 $u,w$，定义 $\|u-w\|_{\infty}=\sup_s |u(s)-w(s)|$。

(2) 计算差的上界：由 $T^{\pi}$ 的定义并利用三角不等式，有

$$
\begin{aligned}
|T^{\pi}u(s)-T^{\pi}w(s)| &= \left|\sum_a \pi(a\mid s)\gamma\sum_{s'} P(s'\mid s,a)\big(u(s')-w(s')\big)\right| \\[4pt]
&\le \gamma \sum_a \pi(a\mid s)\sum_{s'} P(s'\mid s,a)\,|u(s')-w(s')| \\[4pt]
&= \gamma \sum_{s'} P^{\pi}(s'\mid s)\,|u(s')-w(s')| \\[4pt]
&\le \gamma \|u-w\|_{\infty}.
\end{aligned}
$$

(3) 对上式在所有 $s$ 取上确界，得到

$$
\|T^{\pi}u - T^{\pi}w\|_{\infty} \le \gamma \|u-w\|_{\infty}.
$$

(4) 由于 $0\le\gamma<1$，$T^{\pi}$ 在 $(\mathbb{R}^{|S|},\|\cdot\|_{\infty})$ 上是收缩映射。由 Banach 不动点定理（收缩映射原理），收缩映射在完整空间上存在唯一不动点。因此 $T^{\pi}$ 的不动点唯一。

步骤4：在有限 MDP 且 $0\le\gamma<1$ 时的不动点唯一性（方法二：线性代数 / 矩阵可逆性）

（此法对熟悉线性代数的读者作为替代证明）

(1) 在有限状态集下，$P^{\pi}$ 是一个行随机矩阵，因此其谱半径 $\rho(P^{\pi})\le 1$（即所有特征值的模不超过 1）。

(2) 因此 $\rho(\gamma P^{\pi}) = \gamma\,\rho(P^{\pi}) \le \gamma <1$。由谱半径小于 1 可知矩阵 $I-\gamma P^{\pi}$ 可逆，其逆可由 Neumann 级数给出：

$$
(I-\gamma P^{\pi})^{-1} = \sum_{k=0}^{\infty} (\gamma P^{\pi})^{k},
$$

级数收敛是因为 $\|\gamma P^{\pi}\|<1$（在某一矩阵范数下，例如运算范数或谱半径约束下）。

(3) 因此线性方程 $(I-\gamma P^{\pi})v=r^{\pi}$ 有唯一解

$$
v = (I-\gamma P^{\pi})^{-1} r^{\pi} = \sum_{k=0}^{\infty} (\gamma P^{\pi})^{k} r^{\pi}.
$$

(4) 这说明在有限 MDP 且 $0\le\gamma<1$ 时，满足向量等式的解唯一，所以 $T^{\pi}$ 的不动点唯一。

步骤5：结论汇总

(1) 由步骤1-2，任何 $T^{\pi}$ 的不动点都等价于满足贝尔曼期望方程的值函数解。 
(2) 由步骤3（或步骤4）在有限 MDP 且 $0\le\gamma<1$ 时，不动点唯一。

因此证明完毕。
</Proof>

</ExampleItem>

</Exercises>

</Section>

---

<Section id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_a3f5a1e0">
## 第三章：蒙特卡洛与时序差分方法（采样型算法）

<Introduction id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_a3f5a1e0_field_f2aa2c43">
本章介绍基于采样的无模型方法：蒙特卡洛（MC）方法与时序差分（TD）学习及其扩展（SARSA、Q-learning、Eligibility traces、TD(λ)）和离策略学习的基本重要性采样思想。目标是掌握基于经验轨迹估计价值/动作值的核心更新规则、收敛性条件、偏差-方差权衡、实现细节及常用实验/调参建议，为后续引入函数逼近与深度方法打下坚实基础。
</Introduction>

### 总结
<Summary id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_a3f5a1e0_field_82906e7c">
本节练习证明前向 $\lambda$-returns 与后向资格迹（backward eligibility traces）在期望意义下的一致性，给出代数推导与关键恒等式。
</Summary>

<Exercises>
#### 证明题
<ExampleItem id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_a3f5a1e0_example_819b96a2">

**题目：**
<Question id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_a3f5a1e0_example_819b96a2_field_32987d3d">
证明：在期望意义下，后向资格迹的增量更新与前向 $\lambda$-returns 的期望更新等价（给出主要代数步骤）。
</Question>

**答案：**
<Answer id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_a3f5a1e0_example_819b96a2_field_89198612">
结论：在相同初始估计和策略下，对任意状态 $S_t$，后向资格迹的期望累计更新等于前向 $\lambda$-return 的期望更新，即 $\mathbb{E}[\text{backward update}]=\mathbb{E}[G_t^{\lambda}-V(S_t)]$（乘以学习率）。
</Answer>

**证明步骤：**
<Proof id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_a3f5a1e0_example_819b96a2_field_3eb966c6">
**步骤1：定义与记号**

- 定义 $n$ 步回报

$$
G_t^{(n)}=\sum_{k=0}^{n-1}\gamma^{k}R_{t+k+1}+\gamma^{n}V(S_{t+n}).
$$

- 定义单步 TD 误差

$$
\delta_{t}=R_{t+1}+\gamma V(S_{t+1})-V(S_t).
$$

- 定义前向 $\lambda$-返回（对无穷情形用条件收敛）

$$
G_t^{\lambda}=(1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G_t^{(n)}.
$$

**步骤2：证明 $G_t^{(n)}-V(S_t)$ 与 TD 误差的和的关系（分步代数推导）**

我们要证明下面的恒等式：

$$
G_t^{(n)}-V(S_t)=\sum_{k=0}^{n-1}\gamma^{k}\delta_{t+k}.
$$

证明：从 $G_t^{(n)}$ 的定义出发，计算差值：

$$
\begin{aligned}
G_t^{(n)}-V(S_t)&=\left(\sum_{k=0}^{n-1}\gamma^{k}R_{t+k+1}+\gamma^{n}V(S_{t+n})\right)-V(S_t)\\
&=\sum_{k=0}^{n-1}\gamma^{k}R_{t+k+1}+\gamma^{n}V(S_{t+n})-V(S_t).
\end{aligned}
$$

对右端做分项重写，利用逐项添加与减去中间的 $V(S_{t+k})$ 项并因式分组（即“望远镜求和”）：

$$
\begin{aligned}
&\sum_{k=0}^{n-1}\gamma^{k}R_{t+k+1}+\gamma^{n}V(S_{t+n})-V(S_t)\\
&=\sum_{k=0}^{n-1}\gamma^{k}\bigl[R_{t+k+1}+\gamma V(S_{t+k+1})-V(S_{t+k})\bigr]\\
&=\sum_{k=0}^{n-1}\gamma^{k}\delta_{t+k}.
\end{aligned}
$$

这里第一个等号是通过逐项展开并将 $\gamma^{n}V(S_{t+n})$ 与初始的 $-V(S_t)$ 以及中间项配对得出（标准的望远镜展开），第二个等号直接使用了 $\delta_{t+k}$ 的定义。

**步骤3：用前向 $\lambda$-返回的凸组合写出 $G_t^{\lambda}-V(S_t)$ 并交换求和顺序**

由步骤2，

$$
G_t^{(n)}-V(S_t)=\sum_{k=0}^{n-1}\gamma^{k}\delta_{t+k}.
$$

代入前向 $\lambda$-返回的定义并计算 $G_t^{\lambda}-V(S_t)$：

$$
\begin{aligned}
G_t^{\lambda}-V(S_t)&=(1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}\bigl(G_t^{(n)}-V(S_t)\bigr)\\
&=(1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}\sum_{k=0}^{n-1}\gamma^{k}\delta_{t+k}.
\end{aligned}
$$

交换求和顺序（用 Fubini 交换或因为非负权重保证绝对收敛）：对固定的 $k\ge0$，$n$ 的取值从 $k+1$ 到 $\infty$，因此

$$
\begin{aligned}
G_t^{\lambda}-V(S_t)&=(1-\lambda)\sum_{k=0}^{\infty}\gamma^{k}\delta_{t+k}\sum_{n=k+1}^{\infty}\lambda^{n-1}\\
&=(1-\lambda)\sum_{k=0}^{\infty}\gamma^{k}\delta_{t+k}\cdot\lambda^{k}\sum_{m=0}^{\infty}\lambda^{m}\\
&=(1-\lambda)\sum_{k=0}^{\infty}\gamma^{k}\delta_{t+k}\cdot\lambda^{k}\cdot\frac{1}{1-\lambda}\\
&=\sum_{k=0}^{\infty}(\gamma\lambda)^{k}\delta_{t+k}.
\end{aligned}
$$

其中在第二行我们令 $m=n-(k+1)$ 将内和重写为等比级数，第三行用等比级数求和公式 $\sum_{m=0}^{\infty}\lambda^{m}=1/(1-\lambda)$（假设 $|\lambda|<1$ 或在收敛意义下处理）。最终得到：

$$
G_t^{\lambda}-V(S_t)=\sum_{k=0}^{\infty}(\gamma\lambda)^{k}\delta_{t+k}.
$$

**步骤4：后向视角（资格迹）给出相同的加权 TD 误差和**

在后向（backward）视角下，采用累积（accumulating）资格迹并针对初始时刻 $t$ 对 $V(S_t)$ 的总更新（在不讨论学习率因子时）为：

$$
\Delta V(S_t)=\alpha\sum_{k=0}^{\infty}e_{t+k}(S_t)\,\delta_{t+k},
$$

其中资格迹在每一步按规则更新，对于在时间 $t$ 被访问的状态（并假设在访问 $S_t$ 时将其资格迹设为 1），在随后的每一步如果不再被访问且使用常见的衰减规则（替代、累积等中最常见的累积迹的演化）会满足

$$
e_{t+k}(S_t)=(\gamma\lambda)^{k},\qquad k\ge0.
$$

这是由资格迹的递推公式（累积迹的一般形式）

$$
e_{u+1}(s)=\gamma\lambda e_u(s)+\mathbf{1}\{S_{u+1}=s\}
$$

和在时刻 $t$ 刚访问 $s=S_t$ 时有 $e_t(S_t)=1$，以及在随后的步骤若不再访问该状态则指示函数为 0，从而得到递推解 $e_{t+k}= (\gamma\lambda)^k$。

因此后向方法对 $V(S_t)$ 的累计（未乘学习率）更新为

$$
\sum_{k=0}^{\infty}(\gamma\lambda)^{k}\delta_{t+k},
$$

与步骤3 中得到的 $G_t^{\lambda}-V(S_t)$ 完全相同。

**步骤5：取期望并结论**

将上式乘以学习率 $\alpha$ 即为常用更新量；取条件期望（例如在给定 $S_t$ 和策略/估计下，对未来随机性取期望）得：

$$
\mathbb{E}\bigl[\alpha\sum_{k=0}^{\infty}(\gamma\lambda)^{k}\delta_{t+k}\mid S_t\bigr]=\alpha\,\mathbb{E}[G_t^{\lambda}-V(S_t)\mid S_t].
$$

因此在期望意义下，后向资格迹的增量更新与前向 $\lambda$-returns 的期望更新相等，证明完毕。

**备注（理解要点）**：关键恒等式是

$$
G_t^{(n)}-V(S_t)=\sum_{k=0}^{n-1}\gamma^{k}\delta_{t+k}
$$

以及将前向 $\lambda$-返回作为不同 $n$-步回报的加权平均后通过交换求和得到的权重 $ (\gamma\lambda)^k$，再结合资格迹的衰减行为给出完全相同的加权 TD 误差和。
</Proof>

</ExampleItem>

</Exercises>

</Section>

---

<Section id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_956dccf9">
## 第四章：函数逼近与基于价值的深度强化学习

<Introduction id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_956dccf9_field_c3aae202">
本章介绍将函数逼近用于值函数/动作值函数估计的方法与工程实践，包括线性与神经网络表示、逼近导致的不稳定性、线性逼近下的 TD 推导矩阵形式、DQN 及关键改进与训练细节，以及 Atari 级别的预处理与评估要点。
</Introduction>

### 总结
<Summary id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_956dccf9_field_e14b327c">

</Summary>

<Exercises>
#### 证明题
<ExampleItem id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_956dccf9_example_0333bf48">

**题目：**
<Question id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_956dccf9_example_0333bf48_field_8f7b7a8c">
证明：对于线性逼近 $\hat v=\Phi\theta$，TD(0) 的期望单步更新方向为 $b-A\theta$，其中 $A$ 与 $b$ 如上文所定义。
</Question>

**答案：**
<Answer id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_956dccf9_example_0333bf48_field_a81dbf00">
$\mathbb{E}[\Delta\theta]=\alpha(b-A\theta)$，去掉步长因子后方向为 $b-A\theta$。
</Answer>

**证明步骤：**
<Proof id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_956dccf9_example_0333bf48_field_ad25ee7d">
**证明：**

**步骤1：写出单步 TD(0) 的增量定义。**

单步 TD(0) 更新为

$$\Delta\theta=\alpha\,\delta\,\phi(s),$$

其中 $\delta$ 为 TD 误差，定义为

$$\delta=r+\gamma\hat v(s';\theta)-\hat v(s;\theta),$$

且在线性逼近情形下 $\hat v(s;\theta)=\phi(s)^{\top}\theta$。因此

$$\delta=r+\gamma\phi(s')^{\top}\theta-\phi(s)^{\top}\theta.$$ 

**说明：** 这里 $\phi(s)$ 是特征向量，$\theta$ 是参数向量，$\alpha$ 为步长。

**步骤2：写出期望更新（去掉步长因子），利用期望的线性性展开。**

我们关心的是在样本分布下的期望方向，去掉常数因子 $\alpha$ 后有

$$\mathbb{E}\big[\tfrac{1}{\alpha}\Delta\theta\big]=\mathbb{E}\big[\phi(s)\,\delta\big].$$

将 $\delta$ 代入并利用期望的线性性（即 $\mathbb{E}[X+Y]=\mathbb{E}[X]+\mathbb{E}[Y]$ 以及常数可移出期望）得到

$$
\begin{aligned}
\mathbb{E}\big[\phi(s)\,\delta\big]
&=\mathbb{E}\big[\phi(s)\big(r+\gamma\phi(s')^{\top}\theta-\phi(s)^{\top}\theta\big)\big]\\
&=\mathbb{E}\big[\phi(s)r\big]+\gamma\,\mathbb{E}\big[\phi(s)\phi(s')^{\top}\big]\theta-\mathbb{E}\big[\phi(s)\phi(s)^{\top}\big]\theta.
\end{aligned}
$$

**说明：** 在第二行中，$\phi(s')^{\top}\theta$ 是标量，因而可以写成矩阵乘向量的形式，并且可以将 $\theta$ 从期望中提出，因为期望是对样本 $(s,s',r)$ 的随机性取的，而 $\theta$ 在计算期望时视为常数。

**步骤3：用 $A$ 和 $b$ 的定义重写表达式。**

定义

$$b\triangleq\mathbb{E}[\phi(s)r],\qquad A\triangleq\mathbb{E}\big[\phi(s)\big(\phi(s)-\gamma\phi(s')\big)^{\top}\big].$$

展开 $A$ 可得

$$A=\mathbb{E}[\phi(s)\phi(s)^{\top}]-\gamma\,\mathbb{E}[\phi(s)\phi(s')^{\top}].$$

于是上式中的第三项与第二项可以组合为

$$\gamma\,\mathbb{E}[\phi(s)\phi(s')^{\top}]\theta-\mathbb{E}[\phi(s)\phi(s)^{\top}]\theta = -\big(\mathbb{E}[\phi(s)\phi(s)^{\top}]-\gamma\,\mathbb{E}[\phi(s)\phi(s')^{\top}]\big)\theta = -A\theta.$$

**步骤4：得出结论。**

将上述结果代回第 2 步的期望表达式，得到

$$\mathbb{E}\big[\tfrac{1}{\alpha}\Delta\theta\big]=b-A\theta.$$

因此去掉步长因子 $\alpha$ 后，TD(0) 的期望单步更新方向为

$$b-A\theta,$$

即证明完毕。
</Proof>

</ExampleItem>

</Exercises>

</Section>

---

<Section id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_09af19b3">
## 第五章：策略梯度与Actor-Critic方法

<Introduction id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_09af19b3_field_86b56e1b">
本章介绍直接优化策略的策略梯度方法与Actor-Critic混合结构。目标是理解如何以参数化策略为对象直接优化期望回报、掌握无偏或近似的梯度估计（如REINFORCE与对数梯度技巧）、理解方差降低方法（基线、优势估计、归一化回报等）、以及学习Actor-Critic架构及其并行化（A2C/A3C）。随后介绍确定性策略梯度（DDPG）、受约束/裁剪策略优化（TRPO、PPO）等现代方法的核心思想与伪代码概要，并讨论策略优化中的稳定性挑战与实验实践建议。
</Introduction>

### 总结
<Summary id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_09af19b3_field_c4813f8f">
若基线 $b(s)$ 与动作无关，则在策略梯度估计中用 $Q^{\pi}(s,a)-b(s)$ 替换 $Q^{\pi}(s,a)$ 不改变期望；关键在于对动作的期望 $\mathbb{E}_{a\sim\pi_\theta}[\nabla_\theta\log\pi_\theta(a\mid s)]$ 为零。
</Summary>

<Exercises>
#### 证明题
<ExampleItem id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_09af19b3_example_f3ddc6ef">

**题目：**
<Question id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_09af19b3_example_f3ddc6ef_field_f1177683">
证明：若基线 $b(s)$ 不依赖动作，则将 $Q^{\pi}(s,a)$ 替换为 $Q^{\pi}(s,a)-b(s)$ 的策略梯度期望不变。
</Question>

**答案：**
<Answer id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_09af19b3_example_f3ddc6ef_field_3109bb91">
若基线 $b(s)$ 与动作无关，则替换不会改变策略梯度的期望，基线项的期望为零。
</Answer>

**证明步骤：**
<Proof id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_09af19b3_example_f3ddc6ef_field_98fd39c5">
步骤1：写出包含基线的策略梯度期望形式。

根据策略梯度的期望表达式（Policy Gradient Theorem 的经验形式），在使用基线 $b(s)$ 时策略梯度可以写为

$$
\nabla_\theta J(\theta)=\mathbb{E}_{s\sim d^{\pi},\,a\sim\pi_\theta}[\nabla_\theta\log\pi_\theta(a\mid s)\,(Q^{\pi}(s,a)-b(s))].
$$

步骤2：考察基线引入前后差别的项。

将基线项单独写出，基线引入造成的差别为

$$
\Delta = -\mathbb{E}_{s\sim d^{\pi},\,a\sim\pi_\theta}[\nabla_\theta\log\pi_\theta(a\mid s)\,b(s)].
$$

若证明 $\Delta=0$ 即可说明替换不改变期望。

步骤3：将 $b(s)$ 从对动作的期望中提出。

由于 $b(s)$ 不依赖动作 $a$，对 $a$ 的期望可以把 $b(s)$ 提出：

$$
\mathbb{E}_{a\sim\pi_\theta}[\nabla_\theta\log\pi_\theta(a\mid s)\,b(s)] = b(s)\,\mathbb{E}_{a\sim\pi_\theta}[\nabla_\theta\log\pi_\theta(a\mid s)].
$$

因此

$$
\Delta = -\mathbb{E}_{s\sim d^{\pi}}\Big[b(s)\,\mathbb{E}_{a\sim\pi_\theta}[\nabla_\theta\log\pi_\theta(a\mid s)]\Big].
$$

步骤4：证明内层期望在每个状态 $s$ 下为零。

对于离散动作空间，有

$$
\mathbb{E}_{a\sim\pi_\theta}[\nabla_\theta\log\pi_\theta(a\mid s)] = \sum_a \pi_\theta(a\mid s)\,\nabla_\theta\log\pi_\theta(a\mid s).
$$

应用恒等式 $\nabla_\theta\log\pi_\theta(a\mid s)=\dfrac{\nabla_\theta\pi_\theta(a\mid s)}{\pi_\theta(a\mid s)}$，得

$$
\sum_a \pi_\theta(a\mid s)\,\nabla_\theta\log\pi_\theta(a\mid s) = \sum_a \nabla_\theta\pi_\theta(a\mid s) = \nabla_\theta\sum_a \pi_\theta(a\mid s).
$$

由于策略在每一状态下对动作概率的归一化条件 $\sum_a \pi_\theta(a\mid s)=1$ 成立，因此

$$
\nabla_\theta\sum_a \pi_\theta(a\mid s)=\nabla_\theta 1 = 0.
$$

因此对任意状态 $s$，都有

$$
\mathbb{E}_{a\sim\pi_\theta}[\nabla_\theta\log\pi_\theta(a\mid s)]=0.
$$

对于连续动作空间，上述求和替换为积分，推导同样成立：

$$
\mathbb{E}_{a\sim\pi_\theta}[\nabla_\theta\log\pi_\theta(a\mid s)] = \int \pi_\theta(a\mid s)\,\nabla_\theta\log\pi_\theta(a\mid s)\,da = \int \nabla_\theta\pi_\theta(a\mid s)\,da = \nabla_\theta \int \pi_\theta(a\mid s)\,da = \nabla_\theta 1 = 0.
$$

步骤5：结论。

由步骤4，在每一状态 $s$ 内部的期望为零，代回步骤3 的表达式得到 $\Delta=0$。因此

$$
\mathbb{E}_{s,a}[\nabla_\theta\log\pi_\theta(a\mid s)\,(Q^{\pi}(s,a)-b(s))] = \mathbb{E}_{s,a}[\nabla_\theta\log\pi_\theta(a\mid s)\,Q^{\pi}(s,a)].
$$

即当基线 $b(s)$ 不依赖动作时，用 $Q^{\pi}(s,a)-b(s)$ 替换 $Q^{\pi}(s,a)$ 不改变策略梯度的期望，完成证明。
</Proof>

</ExampleItem>

</Exercises>

</Section>

---

<Section id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_9408a643">
## 第六章：实用问题、进阶主题与评估指标

<Introduction id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_9408a643_field_5ad814ba">
本章汇总强化学习从研究到工程落地时常见的实战问题、进阶主题与评估方法。目标是提供可操作的策略（如探索机制、稀疏奖励对策、样本效率提升与离线RL技巧）、稳健性与安全性考虑、实验设计与评估指标、工程实践要点（可复现性、分布式训练、日志监控与自动化调参）以及社区资源，帮助读者在实际项目中有效诊断问题、选择方法并进行可靠评估。
</Introduction>

<Examples>
#### 证明题
<ExampleItem id="ex1">

**题目：**
<Question id="ex1_field_a178cf29">
证明：若奖励塑形使用潜在函数形式 $F(s,a,s')=\gamma\Phi(s')-\Phi(s)$，则对任意策略，累积回报的排序不变（即最优策略不变）。
</Question>

**答案：**
<Answer id="ex1_field_c6c2246c">
潜在函数差形式的塑形只在回报上增加一个与轨迹起止状态有关的可加常数（按折扣后望远项），因此不会改变任何两个策略的比较顺序，最优策略不变。
</Answer>

**证明步骤：**
<Proof id="ex1_field_36e4f1bb">
步骤1：说明设定与记号。

- 设原即时奖励为 $r(s_t,a_t,s_{t+1})$。
- 定义塑形函数为 $F(s_t,a_t,s_{t+1})=\gamma\Phi(s_{t+1})-\Phi(s_t)$，因此塑形后的即时奖励为
  $$r'(s_t,a_t,s_{t+1})=r(s_t,a_t,s_{t+1})+\gamma\Phi(s_{t+1})-\Phi(s_t).$$
- 对于给定策略 $\pi$，考虑一个轨迹 $\tau=(s_0,a_0,s_1,a_1,\dots)$，其折扣回报（从时刻 $0$ 开始）定义为
  $$G(\tau)=\sum_{t=0}^{\infty}\gamma^{t}r(s_t,a_t,s_{t+1}),$$
  塑形后的折扣回报为
  $$G'(\tau)=\sum_{t=0}^{\infty}\gamma^{t}r'(s_t,a_t,s_{t+1}).$$

步骤2：把 $G'(\tau)$ 展开并代入 $r'$ 的定义，展示中间计算并使用求和移位。

- 代入得到
  $$\begin{aligned}
  G'(\tau)&=\sum_{t=0}^{\infty}\gamma^{t}\big(r(s_t,a_t,s_{t+1})+\gamma\Phi(s_{t+1})-\Phi(s_t)\big)\\
  &=\sum_{t=0}^{\infty}\gamma^{t}r(s_t,a_t,s_{t+1})+\sum_{t=0}^{\infty}\gamma^{t+1}\Phi(s_{t+1})-\sum_{t=0}^{\infty}\gamma^{t}\Phi(s_t).
  \end{aligned}$$

- 第一项就是原回报 $G(\tau)$，因此
  $$G'(\tau)=G(\tau)+\sum_{t=0}^{\infty}\gamma^{t+1}\Phi(s_{t+1})-\sum_{t=0}^{\infty}\gamma^{t}\Phi(s_t).$$

步骤3：对两项求和做下标移位并利用望远项（telescoping）消去中间项，给出差值的简单形式。

- 将第一个求和下标换元 $k=t+1$，得到
  $$\sum_{t=0}^{\infty}\gamma^{t+1}\Phi(s_{t+1})=\sum_{k=1}^{\infty}\gamma^{k}\Phi(s_k).$$
- 第二个求和为
  $$\sum_{t=0}^{\infty}\gamma^{t}\Phi(s_t)=\Phi(s_0)+\sum_{k=1}^{\infty}\gamma^{k}\Phi(s_k).$$
- 因此两者相减得到（望远项抵消）
  $$\sum_{t=0}^{\infty}\gamma^{t+1}\Phi(s_{t+1})-\sum_{t=0}^{\infty}\gamma^{t}\Phi(s_t)=-\Phi(s_0).$$

步骤4：写出最终表达并说明含义与假设。

- 将上式代回，得到
  $$G'(\tau)=G(\tau)-\Phi(s_0).$$
- 结论性说明：差值 $G'(\tau)-G(\tau)=-\Phi(s_0)$ 只依赖于轨迹的起始状态 $s_0$，与轨迹中采取的动作序列无关。因此对于任意两个策略 $\pi_1,\pi_2$，若它们从相同初始状态 $s_0$ 出发，则对任意可能的轨迹集合，塑形前后两策略的期望回报只相差同一个常数 $-\Phi(s_0)$，不会改变它们的相对排序，最优策略保持不变。

步骤5：关于边界情况和必要条件的说明。

- 若考虑有限时域至 $T$ 的回报，类似推导得到
  $$G'_T(\tau)=G_T(\tau)-\Phi(s_0)+\gamma^{T+1}\Phi(s_{T+1}),$$
  差别仅与轨迹的起始与终止状态有关；若比较以相同起始状态和相同（或不相关的）终止处理方式的策略，排序仍然保持不变。
- 对于无限时域，需要假设 $\gamma\in[0,1)$ 且函数 $\Phi$ 有界（存在常数 $C$ 使得 $|\Phi(s)|\le C$ 对任意 $s$ 成立），以保证 $\lim_{T\to\infty}\gamma^{T+1}\Phi(s_{T+1})=0$，从而能得到上面无终止项的结论。

综上所述，使用形如 $F(s,a,s')=\gamma\Phi(s')-\Phi(s)$ 的潜在函数进行奖励塑形不会改变策略的排序，最优策略保持不变（在上述覆盖性与有界性假设下成立）。
</Proof>

</ExampleItem>

#### 证明题
<ExampleItem id="ex2">

**题目：**
<Question id="ex2_field_cfc6a569">
证明：使用重要性采样估计期望回报时，基于行为策略 $b$ 采样并用权重 $w(\tau)=\prod_{t}\frac{\pi(a_t|s_t)}{b(a_t|s_t)}$ 的普通重要性采样估计器对目标策略 $\pi$ 的期望是无偏的，即 $\mathbb{E}_{\tau\sim b}[w(\tau)R(\tau)]=\mathbb{E}_{\tau\sim\pi}[R(\tau)]$。
</Question>

**答案：**
<Answer id="ex2_field_e59fbf5b">
重要性采样估计器是无偏的：因为轨迹概率之比将行为策略下的期望转换为目标策略下的期望。
</Answer>

**证明步骤：**
<Proof id="ex2_field_457190e6">
步骤1：设定与记号。

- 令轨迹为 $\tau=(s_0,a_0,s_1,a_1,\dots,s_T)$（可为终止时刻为 $T$ 的情形），轨迹对应的回报记为 $R(\tau)$（可为折扣回报或累积回报）。
- 在行为策略 $b$ 下生成轨迹的概率为
  $$p_b(\tau)=p(s_0)\prod_{t=0}^{T-1}b(a_t|s_t)p(s_{t+1}|s_t,a_t),$$
  在目标策略 $\pi$ 下为
  $$p_\pi(\tau)=p(s_0)\prod_{t=0}^{T-1}\pi(a_t|s_t)p(s_{t+1}|s_t,a_t),$$
  其中 $p(s_0)$ 为初始状态分布，$p(s_{t+1}|s_t,a_t)$ 为环境转移概率，两者与策略无关。

步骤2：给出重要性权重的等价形式并说明转移概率的抵消。

- 权重定义为
  $$w(\tau)=\prod_{t=0}^{T-1}\frac{\pi(a_t|s_t)}{b(a_t|s_t)}.$$
- 注意到
  $$\frac{p_\pi(\tau)}{p_b(\tau)}=\frac{p(s_0)\prod_{t=0}^{T-1}\pi(a_t|s_t)p(s_{t+1}|s_t,a_t)}{p(s_0)\prod_{t=0}^{T-1}b(a_t|s_t)p(s_{t+1}|s_t,a_t)}=\prod_{t=0}^{T-1}\frac{\pi(a_t|s_t)}{b(a_t|s_t)}=w(\tau),$$
  因为初始分布 $p(s_0)$ 与转移概率 $p(s_{t+1}|s_t,a_t)$ 在分子分母中完全抵消。

步骤3：用全概率求和验证无偏性，通过代数变换将 $\mathbb{E}_{\tau\sim b}[w(\tau)R(\tau)]$ 转为关于 $p_\pi$ 的期望。

- 在行为策略下的期望为
  $$\mathbb{E}_{\tau\sim b}[w(\tau)R(\tau)]=\sum_{\tau}p_b(\tau)w(\tau)R(\tau).$$
- 代入 $w(\tau)=p_\pi(\tau)/p_b(\tau)$ 得
  $$\sum_{\tau}p_b(\tau)\frac{p_\pi(\tau)}{p_b(\tau)}R(\tau)=\sum_{\tau}p_\pi(\tau)R(\tau)=\mathbb{E}_{\tau\sim\pi}[R(\tau)].$$

步骤4：指出必要的可行性条件与边界情况。

- 上述代换在数学上要求当 $p_\pi(\tau)>0$ 时必须有 $p_b(\tau)>0$，即行为策略 $b$ 必须在所有目标策略可能产生的动作上有正概率（常称为覆盖性或支持条件：对于任意 $s,a$ 若 $\pi(a|s)>0$ 则需 $b(a|s)>0$），否则比值 $w(\tau)$ 在某些轨迹上未定义或导致偏差。
- 若满足该支持条件且求和/积分收敛，则上述推导严格成立，普通重要性采样估计器关于 $\pi$ 的期望为无偏估计。

结论：在满足覆盖性假设下，使用权重 $w(\tau)=\prod_t\frac{\pi(a_t|s_t)}{b(a_t|s_t)}$ 的普通重要性采样估计器满足
$$\mathbb{E}_{\tau\sim b}[w(\tau)R(\tau)]=\mathbb{E}_{\tau\sim\pi}[R(\tau)],$$
即该估计器无偏。
</Proof>

</ExampleItem>

</Examples>

### 总结
<Summary id="d9198e26-8450-44b0-bb2a-c8b6daf482d0_section_9408a643_field_f38dee2c">

</Summary>

</Section>

---

